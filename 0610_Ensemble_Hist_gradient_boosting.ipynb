{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1226aa",
   "metadata": {},
   "source": [
    "# Speeding-up gradient-boosting\n",
    "In this notebook, we present a **modified version of gradient boosting** which\n",
    "uses a **reduced number of splits** when building the different trees. This\n",
    "algorithm is called **\"histogram gradient boosting\"** in scikit-learn.\n",
    "\n",
    "We previously mentioned that **random-forest** is an **efficient algorithm** since\n",
    "**trees in the ensemble** can be **fitted parallel**.\n",
    "Therefore, the algorithm **scales efficiently** with both the **number of cores** and\n",
    "the **number of samples**.\n",
    "\n",
    "In **gradient-boosting**, the algorithm is a **sequential algorithm**. It requires\n",
    "the `N-1` trees to have been fit to be able to fit the tree at stage `N`.\n",
    "Therefore, the algorithm is computationally expensive. The most\n",
    "**expensive part** in this algorithm is the **search for the best split** in the\n",
    "tree which is a **brute-force** approach: **all possible split are evaluated** and\n",
    "the **best one is picked**. We explained this process in the notebook \"tree in\n",
    "depth\", which you can refer to.\n",
    "\n",
    "**To accelerate the gradient-boosting** algorithm, one could **reduce the number of\n",
    "splits to be evaluated**. As a consequence, the statistical performance of such\n",
    "a tree would be reduced. However, since we are combining several trees in a\n",
    "gradient-boosting, we can add more estimators to overcome this issue.\n",
    "\n",
    "We will make a **naive implementation of such algorithm** using building blocks\n",
    "from scikit-learn. First, we will load the **California housing dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3228c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd203fda",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852acf4",
   "metadata": {},
   "source": [
    "We will make a quick **benchmark of the original gradient boosting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7ad501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradient_boosting = GradientBoostingRegressor(n_estimators=200)\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b80a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Decision Tree\n",
      "Mean absolute error via cross-validation: 46.392 +/- 2.911 k$\n",
      "Average fit time: 6.746 seconds\n",
      "Average score time: 0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Decision Tree\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d7a63",
   "metadata": {},
   "source": [
    "We recall that a way of accelerating the gradient boosting is to reduce the\n",
    "number of split considered within the tree building. One way is to **bin the\n",
    "data before feeding it to the gradient boosting algorithm**. A **transformer** called\n",
    "**`KBinsDiscretizer`** is doing such transformation. Thus, we can **pipeline**\n",
    "this preprocessing **with the gradient boosting**.\n",
    "\n",
    "We can first demonstrate the transformation done by the `KBinsDiscretizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ee951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:220: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:220: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:220: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:220: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn('Bins whose width are too small (i.e., <= '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[249.,  39., 231., ...,  83., 162.,  30.],\n",
       "       [248.,  19., 203., ...,  28., 161.,  30.],\n",
       "       [242.,  49., 249., ..., 125., 160.,  29.],\n",
       "       ...,\n",
       "       [ 17.,  15., 126., ...,  49., 200.,  82.],\n",
       "       [ 23.,  16., 136., ...,  29., 200.,  77.],\n",
       "       [ 53.,  14., 130., ...,  93., 199.,  81.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=256, encode=\"ordinal\", strategy=\"quantile\")\n",
    "data_trans = discretizer.fit_transform(data)\n",
    "data_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f09ab2",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">The code cell above will generate a couple of warnings. Indeed, for some of\n",
    "the features, we requested too many bins in regard of the data dispersion\n",
    "for those features. The smallest bins will be removed.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201da05f",
   "metadata": {},
   "source": [
    "We see that the **discretizer transforms** the original data **into an integer**.\n",
    "This integer represents the **bin index** when the **distribution by quantile** is\n",
    "performed. We can **check the number of bins per feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a43bf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256, 50, 256, 253, 256, 256, 207, 235]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(np.unique(col)) for col in data_trans.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0be6",
   "metadata": {},
   "source": [
    "After this transformation, we see that we have **at most 256 unique values per\n",
    "feature**. Now, we will use this transformer to discretize data before\n",
    "training the gradient boosting regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed2eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "gradient_boosting = make_pipeline(\n",
    "    discretizer, GradientBoostingRegressor(n_estimators=200))\n",
    "cv_results_gbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492dbf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Decision Tree with KBinsDiscretizer\n",
      "Mean absolute error via cross-validation: 46.140 +/- 2.231 k$\n",
      "Average fit time: 4.275 seconds\n",
      "Average score time: 0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Boosting Decision Tree with KBinsDiscretizer\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_gbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268863a3",
   "metadata": {},
   "source": [
    "The time to fit has been drastically reduced but the\n",
    "statistical performance of the model is identical. **Scikit-learn** provides\n",
    "specific **classes** which are even more **optimized for large dataset**,\n",
    "**`HistGradientBoostingClassifier`** and **`HistGradientBoostingRegressor`**. **Each\n",
    "feature** in the dataset `data` is first **binned by computing histograms**, which\n",
    "are **later used to evaluate the potential splits**. The **number of splits to\n",
    "evaluate is then much smaller**. This algorithm becomes **much more efficient**\n",
    "than gradient boosting **when** the dataset has **over 10,000 samples**.\n",
    "\n",
    "Below we give an **example for a large dataset** and we will compare\n",
    "computation times with the experiment of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458c03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "histogram_gradient_boosting = HistGradientBoostingRegressor(\n",
    "    max_iter=200, random_state=0)\n",
    "cv_results_hgbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f17208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Gradient Boosting Decision Tree\n",
      "Mean absolute error via cross-validation: 45.816 +/- 2.196 k$\n",
      "Average fit time: 4.273 seconds\n",
      "Average score time: 0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Histogram Gradient Boosting Decision Tree\")\n",
    "print(f\"Mean absolute error via cross-validation: \"\n",
    "      f\"{-cv_results_hgbdt['test_score'].mean():.3f} +/- \"\n",
    "      f\"{cv_results_hgbdt['test_score'].std():.3f} k$\")\n",
    "print(f\"Average fit time: \"\n",
    "      f\"{cv_results_hgbdt['fit_time'].mean():.3f} seconds\")\n",
    "print(f\"Average score time: \"\n",
    "      f\"{cv_results_hgbdt['score_time'].mean():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec3e4e",
   "metadata": {},
   "source": [
    "The **histogram gradient-boosting** is the **best** algorithm in terms of **score**.\n",
    "It will also **scale when the number of samples increases**, while the normal\n",
    "**gradient-boosting will not**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa641a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "nbreset": "https://github.com/INRIA/scikit-learn-mooc/raw/master/notebooks/ensemble_hist_gradient_boosting.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
